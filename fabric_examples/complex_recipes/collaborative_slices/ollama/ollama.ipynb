{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy GPU-Accelerated Ollama LLM Server on FABRIC\n",
    "\n",
    "This notebook demonstrates deploying a complete Large Language Model (LLM) inference server using Ollama on FABRIC's research infrastructure. Following FABRIC's standard deployment patterns, it provisions a GPU-enabled slice and configures a production-ready AI service accessible across the testbed.\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "**Infrastructure Provisioning:**\n",
    "- Automatically selects an optimal FABRIC site with available GPU resources (RTX6000, Tesla T4, A30, or A40)\n",
    "- Creates a slice with GPU-accelerated compute node connected to FABNetv4 for inter-slice communication\n",
    "- Configures Docker with NVIDIA runtime support for containerized AI workloads\n",
    "\n",
    "**LLM Server Deployment:**\n",
    "- Installs and configures Ollama natively with the `deepseek-r1:7b` model (customizable to other models)\n",
    "- Sets up Open-WebUI for browser-based interaction with the LLM\n",
    "- Enables secure remote access via SSH tunneling following FABRIC security practices\n",
    "\n",
    "**Cross-Slice Integration:**\n",
    "- Configures the Ollama API server to accept connections from other FABRIC slices via FABNetv4\n",
    "- Provides both REST API and web interface access for flexible integration with research workflows\n",
    "- Demonstrates querying patterns for distributed AI applications across FABRIC infrastructure\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "- **AI Research**: Deploy custom LLMs for distributed machine learning experiments\n",
    "- **Multi-Slice Applications**: Provide AI services to other FABRIC experiments via FABNetv4 networking\n",
    "- **Educational Demonstrations**: Showcase GPU-accelerated AI deployment on research infrastructure\n",
    "- **Prototype Development**: Test AI applications before scaling to production environments\n",
    "\n",
    "This example follows FABRIC's jupyter-examples patterns and can be adapted for different LLM models, GPU types, or integrated with other FABRIC services like monitoring via MFLib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FABlib Documentation Reference\n",
    "\n",
    "This notebook demonstrates many key FABlib classes and methods for FABRIC resource management:\n",
    "\n",
    "### Core Classes\n",
    "- **[FablibManager](https://fabric-fablib.readthedocs.io/en/latest/fablib.html)**: Main interface for FABRIC operations\n",
    "- **[Slice](https://fabric-fablib.readthedocs.io/en/latest/slice.html)**: Container for experiment resources\n",
    "- **[Node](https://fabric-fablib.readthedocs.io/en/latest/node.html)**: Individual compute resources (VMs)\n",
    "- **[Interface](https://fabric-fablib.readthedocs.io/en/latest/interface.html)**: Network interfaces for connectivity\n",
    "\n",
    "### Key Methods Used\n",
    "- **Resource Discovery**: [`list_hosts()`](https://fabric-fablib.readthedocs.io/en/latest/fablib.html#fabrictestbed_extensions.fablib.fablib.FablibManager.list_hosts)\n",
    "- **Slice Management**: [`new_slice()`](https://fabric-fablib.readthedocs.io/en/latest/fablib.html#fabrictestbed_extensions.fablib.fablib.fablib.new_slice), [`get_slice()`](https://fabric-fablib.readthedocs.io/en/latest/fablib.html#fabrictestbed_extensions.fablib.fablib.fablib.get_slice)\n",
    "- **Node Operations**: [`add_node()`](https://fabric-fablib.readthedocs.io/en/latest/slice.html#fabrictestbed_extensions.fablib.slice.Slice.add_node), [`execute()`](https://fabric-fablib.readthedocs.io/en/latest/node.html#fabrictestbed_extensions.fablib.node.Node.execute)\n",
    "- **Networking**: [`add_l3network()`](https://fabric-fablib.readthedocs.io/en/latest/slice.html#fabrictestbed_extensions.fablib.slice.Slice.add_l3network), [`get_interface()`](https://fabric-fablib.readthedocs.io/en/latest/node.html#fabrictestbed_extensions.fablib.node.Node.get_interface)\n",
    "- **Components**: [`add_component()`](https://fabric-fablib.readthedocs.io/en/latest/node.html#fabrictestbed_extensions.fablib.node.Node.add_component) for GPUs and NICs\n",
    "- **Automation**: [`add_post_boot_upload_directory()`](https://fabric-fablib.readthedocs.io/en/latest/node.html#fabrictestbed_extensions.fablib.node.Node.add_post_boot_upload_directory), [`add_post_boot_execute()`](https://fabric-fablib.readthedocs.io/en/latest/node.html#fabrictestbed_extensions.fablib.node.Node.add_post_boot_execute)\n",
    "\n",
    "### Complete Documentation\n",
    "For comprehensive API reference, visit:\n",
    "**[FABRIC FABlib Documentation](https://fabric-fablib.readthedocs.io/en/latest/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the FABlib Library\n",
    "\n",
    "Initialize the [FablibManager](https://fabric-fablib.readthedocs.io/en/latest/fablib.html) - the core class for managing FABRIC resources and operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ipaddress import ip_address, IPv4Address, IPv6Address, IPv4Network, IPv6Network\n",
    "import ipaddress\n",
    "from fabrictestbed_extensions.fablib.fablib import FablibManager as fablib_manager\n",
    "\n",
    "fablib = fablib_manager()\n",
    "\n",
    "fablib.show_config();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create the Experiment Slice\n",
    "\n",
    "This section provisions a FABRIC slice for GPU-accelerated LLM deployment following the standard resource allocation workflow. The process includes:\n",
    "\n",
    "1. **Site Discovery**: Identifies FABRIC sites with available GPU resources matching our requirements\n",
    "2. **Slice Provisioning**: Creates a new slice with GPU-enabled compute nodes \n",
    "3. **Network Configuration**: Establishes FABNetv4 connectivity for inter-slice communication\n",
    "4. **Automated Setup**: Configures post-boot scripts for Docker and NVIDIA driver installation\n",
    "\n",
    "The slice will contain a single node optimized for AI workloads, with the selected GPU (RTX6000, Tesla T4, A30, or A40) attached and connected to FABRIC's research network infrastructure. Post-boot automation ensures the node is ready for native Ollama installation with full GPU acceleration support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_slice_name = 'Ollama-slice'\n",
    "\n",
    "ollama_node_name ='ollama_node'\n",
    "\n",
    "network_name='net1'\n",
    "nic_name = 'nic1'\n",
    "model_name = 'NIC_Basic'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a Site\n",
    "\n",
    "Configure resource requirements and site preferences for the Ollama server deployment. This section defines the minimum hardware specifications needed to support GPU-accelerated LLM inference and establishes site selection criteria following FABRIC's standard allocation patterns.\n",
    "\n",
    "Set minimum resource thresholds for:\n",
    "- **CPU cores** and **RAM** for LLM processing workloads\n",
    "- **Disk space** for model storage and container images  \n",
    "- **GPU availability** for accelerated inference (optional for initial filtering)\n",
    "\n",
    "Optionally specify site preferences using `sites_prefer` for prioritized locations or `sites_avoid` to exclude sites with known constraints or maintenance issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If empty -> do not filter by name\n",
    "sites_prefer: list[str] = []  # e.g., ['BRIST', 'TOKY'] or [] to disable\n",
    "sites_avoid: list[str] = [\"TACC\", \"CIEN\"]   # e.g., ['BRIST', 'TOKY'] or [] to disable\n",
    "min_cores = 4\n",
    "min_ram_gb = 16\n",
    "min_disk_gb = 200\n",
    "min_gpu_any = 0       # >0 means at least one GPU of any model for the initial filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Host Selection Algorithm\n",
    "\n",
    "The following cell implements a smart host selection algorithm that:\n",
    "\n",
    "1. **Filters available hosts** using [`fablib.list_hosts()`](https://fabric-fablib.readthedocs.io/en/latest/fablib.html#fabrictestbed_extensions.fablib.fablib.FablibManager.list_hosts) based on resource requirements (cores, RAM, disk) and GPU availability\n",
    "2. **Applies site preferences** - you can specify preferred sites or sites to avoid using the `avoid` parameter\n",
    "3. **Randomly selects** from eligible hosts to distribute load across the FABRIC infrastructure\n",
    "4. **Identifies the best GPU type** available on the selected host from RTX6000, Tesla T4, A30, or A40 models\n",
    "\n",
    "This approach ensures your slice gets allocated to a site with sufficient resources while respecting any site constraints you've configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "gpu_models = { 'rtx6000_available': \"GPU_RTX6000\",\n",
    "               'tesla_t4_available': \"GPU_TeslaT4\",\n",
    "               'a30_available': \"GPU_A30\",\n",
    "               'a40_available': \"GPU_A40\"}\n",
    "\n",
    "fields = ['name', 'state', 'cores_available', 'ram_available', 'disk_available']\n",
    "fields = fields + list(gpu_models.keys())\n",
    "\n",
    "def filter_function(row: dict) -> bool:\n",
    "    # Name filter: only apply if sites_prefer is non-empty\n",
    "    if sites_prefer:\n",
    "        name = (row.get('name') or '')\n",
    "        name_ok = any(tok.lower() in name.lower() for tok in sites_prefer)\n",
    "    else:\n",
    "        name_ok = True\n",
    "\n",
    "    res_ok = (\n",
    "        row.get('cores_available', 0) > min_cores and\n",
    "        row.get('ram_available', 0) > min_ram_gb and\n",
    "        row.get('disk_available', 0) > min_disk_gb and\n",
    "        row.get('state') == 'Active'\n",
    "    )\n",
    "    any_gpu_ok = any(row.get(gf, 0) > min_gpu_any for gf in gpu_models.keys())\n",
    "\n",
    "    return name_ok and res_ok and any_gpu_ok\n",
    "\n",
    "hosts = fablib.list_hosts(fields=fields, \n",
    "                            pretty_names=False, \n",
    "                            avoid=sites_avoid, \n",
    "                            filter_function=filter_function,\n",
    "                            output='list',\n",
    "                            quiet=True)\n",
    "\n",
    "\n",
    "host = random.choice(hosts)\n",
    "\n",
    "picked_gpu_key = next((gf for gf in gpu_models.keys() if host[gf] > 0), None)\n",
    "picked_gpu_model = gpu_models.get(picked_gpu_key) \n",
    "picked_gpu_count = host.get(picked_gpu_key, 0)\n",
    "\n",
    "picked_host = host['name']\n",
    "picked_site = picked_host.split('-', 1)[0].upper()\n",
    "\n",
    "print(\n",
    "    f\"Chosen Host: {host['name']} | \"\n",
    "    f\"GPU: {picked_gpu_model} | Available: {picked_gpu_count}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Slice  \n",
    "\n",
    "This cell provisions the FABRIC slice following the standard FABRIC deployment pattern using key FABlib methods:\n",
    "\n",
    "1. **Creates a new slice** using [`fablib.new_slice()`](https://fabric-fablib.readthedocs.io/en/latest/fablib.html#fabrictestbed_extensions.fablib.fablib.fablib.new_slice) named 'Ollama-slice'\n",
    "2. **Establishes FABNetv4 connectivity** by adding an L3 network using [`slice.add_l3network()`](https://fabric-fablib.readthedocs.io/en/latest/slice.html#fabrictestbed_extensions.fablib.slice.Slice.add_l3network) for inter-slice communication\n",
    "3. **Provisions a GPU-enabled node** with [`slice.add_node()`](https://fabric-fablib.readthedocs.io/en/latest/slice.html#fabrictestbed_extensions.fablib.slice.Slice.add_node), applying the minimum resource requirements (cores, RAM, disk)\n",
    "4. **Attaches the chosen GPU** using [`node.add_component()`](https://fabric-fablib.readthedocs.io/en/latest/node.html#fabrictestbed_extensions.fablib.node.Node.add_component) (RTX6000, Tesla T4, A30, or A40) to support AI/LLM workloads\n",
    "5. **Connects to FABNetv4** via a NIC_Basic component configured in auto mode using [`interface.set_mode()`](https://fabric-fablib.readthedocs.io/en/latest/interface.html#fabrictestbed_extensions.fablib.interface.Interface.set_mode) for seamless networking\n",
    "6. **Uploads deployment tools** using [`node.add_post_boot_upload_directory()`](https://fabric-fablib.readthedocs.io/en/latest/node.html#fabrictestbed_extensions.fablib.node.Node.add_post_boot_upload_directory) including ollama_tools and node_tools directories\n",
    "7. **Configures post-boot automation** with [`node.add_post_boot_execute()`](https://fabric-fablib.readthedocs.io/en/latest/node.html#fabrictestbed_extensions.fablib.node.Node.add_post_boot_execute) to enable Docker and install NVIDIA dependencies for GPU support\n",
    "8. **Submits the slice** for provisioning using [`slice.submit()`](https://fabric-fablib.readthedocs.io/en/latest/slice.html#fabrictestbed_extensions.fablib.slice.Slice.submit)\n",
    "\n",
    "The slice follows FABRIC's standard pattern for AI infrastructure deployment, ensuring the node is ready for native Ollama installation with GPU acceleration and cross-slice connectivity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Slice\n",
    "ollama_slice = fablib.new_slice(name=ollama_slice_name)\n",
    "\n",
    "net1 = ollama_slice.add_l3network(name=network_name)\n",
    "\n",
    "ollama_node = ollama_slice.add_node(name=ollama_node_name, cores=min_cores, ram=min_ram_gb, host=picked_host,\n",
    "                                    disk=min_disk_gb, site=picked_site, image='default_ubuntu_22')\n",
    "\n",
    "ollama_node.add_component(model=picked_gpu_model, name='gpu1')\n",
    "\n",
    "\n",
    "iface1 = ollama_node.add_component(model=model_name, name=nic_name).get_interfaces()[0]\n",
    "iface1.set_mode('auto')\n",
    "net1.add_interface(iface1)\n",
    "\n",
    "ollama_node.add_post_boot_upload_directory('ollama_tools','.')\n",
    "ollama_node.add_post_boot_upload_directory('node_tools','.')\n",
    "ollama_node.add_post_boot_execute('node_tools/enable_docker.sh {{ _self_.image }} ')\n",
    "ollama_node.add_post_boot_execute('node_tools/dependencies.sh {{ _self_.image }} ')\n",
    "\n",
    "ollama_slice.submit();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Configure Ollama\n",
    "\n",
    "This section installs Ollama directly on the Ubuntu node and configures it to accept connections from remote hosts. This is an alternative to the Docker-based approach.\n",
    "\n",
    "Users can specify alternative models such as:  \n",
    "\n",
    "`llama2-7b`, `mistral-7b`, `gemma-7b`, `deepseek-r1:67b`, `phi-2`, `gpt-neo-2.7b`  \n",
    "\n",
    "For more available models, visit: [Ollama Model Search](https://ollama.com/search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_llm_model = \"deepseek-r1:7b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconnect to Existing Slice\n",
    "\n",
    "If reconnecting to an existing slice (e.g., after restarting your notebook), use [`fablib.get_slice()`](https://fabric-fablib.readthedocs.io/en/latest/fablib.html#fabrictestbed_extensions.fablib.fablib.fablib.get_slice) and [`slice.get_node()`](https://fabric-fablib.readthedocs.io/en/latest/slice.html#fabrictestbed_extensions.fablib.slice.Slice.get_node) to retrieve existing resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_slice = fablib.get_slice(name=ollama_slice_name)\n",
    "ollama_node = ollama_slice.get_node(ollama_node_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Configure Ollama\n",
    "\n",
    "This section installs Ollama natively on the Ubuntu node using the official installer and configures it to accept connections from remote hosts. This approach provides optimal performance by installing Ollama directly on the system rather than using containers.\n",
    "\n",
    "**Configuration includes:**\n",
    "- Native Ollama installation with GPU acceleration support\n",
    "- Systemd service configuration for remote access\n",
    "- Firewall setup for API connectivity\n",
    "- Model download and initialization\n",
    "\n",
    "All configuration steps use the [`node.execute()`](https://fabric-fablib.readthedocs.io/en/latest/node.html#fabrictestbed_extensions.fablib.node.Node.execute) method to run commands on the remote FABRIC node.\n",
    "\n",
    "### Install Ollama using the official installer\n",
    "\n",
    "Download and install Ollama natively on Ubuntu. This provides better performance than containerized approaches and direct access to GPU resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout, stderr = ollama_node.execute(\"curl -fsSL https://ollama.com/install.sh | sh\", quiet=True, output_file=f\"{ollama_node.get_name()}.log\")\n",
    "print(\"Installation output:\")\n",
    "print(stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Ollama to accept remote connections\n",
    "\n",
    "By default, Ollama only listens on localhost. Configure it for remote access by creating a systemd override to bind to all interfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, Ollama only listens on localhost. We need to configure it for remote access.\n",
    "\n",
    "# Create systemd override directory\n",
    "stdout, stderr = ollama_node.execute(\"sudo mkdir -p /etc/systemd/system/ollama.service.d/\")\n",
    "\n",
    "# Create override configuration to bind to all interfaces\n",
    "override_config = \"\"\"[Service]\n",
    "Environment=\"OLLAMA_HOST=0.0.0.0:11434\"\n",
    "\"\"\"\n",
    "\n",
    "# Write the override file\n",
    "stdout, stderr = ollama_node.execute(f'echo \\'{override_config}\\' | sudo tee /etc/systemd/system/ollama.service.d/override.conf')\n",
    "print(\"Override configuration created:\")\n",
    "print(stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload systemd and restart Ollama service\n",
    "\n",
    "Apply the configuration changes by reloading systemd, restarting the Ollama service, and enabling it to start automatically on boot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout, stderr = ollama_node.execute(\"sudo systemctl daemon-reload\")\n",
    "stdout, stderr = ollama_node.execute(\"sudo systemctl restart ollama\")\n",
    "stdout, stderr = ollama_node.execute(\"sudo systemctl enable ollama\")\n",
    "\n",
    "# Check service status\n",
    "stdout, stderr = ollama_node.execute(\"sudo systemctl status ollama\")\n",
    "print(\"Ollama service status:\")\n",
    "print(stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure firewall to allow connections on port 11434\n",
    "\n",
    "Open the firewall to allow incoming connections on port 11434, which is the default port for the Ollama API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout, stderr = ollama_node.execute(\"sudo ufw allow 11434/tcp\", quiet=True)\n",
    "print(\"Firewall configuration:\")\n",
    "print(stdout);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull the desired model\n",
    "\n",
    "Download the specified LLM model and verify it's available for use. This may take several minutes depending on the model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout, stderr = ollama_node.execute(f\"ollama pull {default_llm_model}\", quiet=True, output_file=f\"{ollama_node.get_name()}.log\")\n",
    "print(f\"Pulling model {default_llm_model}:\")\n",
    "print(stdout)\n",
    "\n",
    "# List available models\n",
    "stdout, stderr = ollama_node.execute(\"ollama list\")\n",
    "print(\"Available models:\")\n",
    "print(stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test local API access\n",
    "\n",
    "Verify that the Ollama API is running and accessible by testing the connection locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test API locally first\n",
    "print(\"Local API test:\")\n",
    "stdout, stderr = ollama_node.execute(\"curl -s http://localhost:11434/api/tags\", quiet=True)\n",
    "print(stdout);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Remote Query\n",
    "\n",
    "Now test querying Ollama from a remote host using curl. The node should be accessible via its FABNetv4 IP address from other FABRIC slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Test a simple query to the model\n",
    "test_query = '''curl -X POST http://localhost:11434/api/generate \\\\\n",
    "  -H \"Content-Type: application/json\" \\\\\n",
    "  -d '{\n",
    "    \"model\": \"''' + default_llm_model + '''\",\n",
    "    \"prompt\": \"Tell me a joke about computer networks.\",\n",
    "    \"stream\": false\n",
    "  }' '''\n",
    "\n",
    "print(test_query)\n",
    "\n",
    "stdout, stderr = ollama_node.execute(test_query, quiet=True)\n",
    "print(\"Query test result:\")\n",
    "# Parse the JSON response\n",
    "response = json.loads(stdout)\n",
    "\n",
    "# Print all keys and values\n",
    "for key, value in response.items():\n",
    "  print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable Access to Ollama Node Across FABRIC  \n",
    "\n",
    "Configure the `ollamanode` to be accessible from any VM running across FABRIC on FabNetV4 by setting up the necessary routes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the FabNet IP Address  \n",
    "Display the FabNet IP address of the Ollama node for sharing with other slices using [`node.get_interface()`](https://fabric-fablib.readthedocs.io/en/latest/node.html#fabrictestbed_extensions.fablib.node.Node.get_interface) and [`interface.get_ip_addr()`](https://fabric-fablib.readthedocs.io/en/latest/interface.html#fabrictestbed_extensions.fablib.interface.Interface.get_ip_addr)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_fabnet_ip_addr = ollama_node.get_interface(network_name=network_name).get_ip_addr()\n",
    "\n",
    "print(f\"Ollama is accessible from other slices at: {ollama_fabnet_ip_addr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying Ollama\n",
    "\n",
    "Users can interact with the LLM through the the command-line interface, REST API, or and Open WebUI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLI Examples\n",
    "\n",
    "SSH into the `ollama_node` using the command provided above.\n",
    "To view available models, run:\n",
    "\n",
    "```bash\n",
    "ollama run deepseek-r1:7b  \"Tell me a joke about computers\"\n",
    "```\n",
    "\n",
    "Alternatively, you can run the command using the FABlib API:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout, stderr = ollama_node.execute(f'ollama run {default_llm_model} \"Tell me a joke about computers\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### REST Examples\n",
    "\n",
    "The `query.py` script demonstrates how to query the LLM over the REST interface. Although Ollama can run on a remote host, the example below targets the local instance by passing `--host localhost`. Users may also specify a different `--host` and `--port` as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout, stderr = ollama_node.execute(f'python3 ollama_tools/query.py --host {ollama_fabnet_ip_addr} --model {default_llm_model} --prompt \"Tell me a joke about computers\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Web UI\n",
    "\n",
    "To access the Open Web UI from your laptop, youâ€™ll need to start the Open WebUI server in a Docker container, create an SSH tunnel from your laptop, and connect to the server using a browser on your laptop.\n",
    "\n",
    "Follow the steps below to complete the setup.\n",
    "\n",
    "#### Start the Open WebUI Server\n",
    "\n",
    "The required docker compose files where included in the post boot upload. The following command will start the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_node.execute(f\"cd ollama_tools && cp env.template .env && docker compose up -d\",\n",
    "                   quiet=True, ouput_file=f\"{ollama_node.get_name()}.log\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start the SSH Tunnel\n",
    "\n",
    "- Create SSH Tunnel Configuration `fabric_ssh_tunnel_tools.zip` using [`fablib.create_ssh_tunnel_config()`](https://fabric-fablib.readthedocs.io/en/latest/fablib.html#fabrictestbed_extensions.fablib.fablib.FablibManager.create_ssh_tunnel_config)\n",
    "- Download your custom `fabric_ssh_tunnel_tools.zip` tarball from the `fabric_config` folder.  \n",
    "- Untar the tarball and put the resulting folder (`fabric_ssh_tunnel_tools`) somewhere you can access it from the command line.\n",
    "- Open a terminal window. (Windows: use `powershell`) \n",
    "- Use `cd` to navigate to the `fabric_ssh_tunnel_tools` folder.\n",
    "- In your terminal, run the command that results from running the following cell (leave the terminal window open)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fablib.create_ssh_tunnel_config(overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to the Open Web UI\n",
    "\n",
    "To access the Open Web UI running on the ollama node, create an SSH tunnel from your local machine using the command generated by the next cell:\n",
    "\n",
    "```bash\n",
    "ssh -L 8080:<manager-ip>:8080 -i <private_key> -F <ssh_config> <your-username>@<manager-host>\n",
    "```\n",
    "\n",
    "Replace `<manager-ip>` and `<manager-host>` with the actual IP address and hostname of the Ceph manager VM.\n",
    "\n",
    "Then, open your browser and navigate to:\n",
    "\n",
    "\n",
    "http://localhost:8080\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Port on your local machine that you want to map the File Browser to.\n",
    "local_port='8080'\n",
    "# Local interface to map the File Browser to (can be `localhost`)\n",
    "local_host='127.0.0.1'\n",
    "\n",
    "# Port on the node used by the File Browser Service\n",
    "target_port='8080'\n",
    "\n",
    "# Username/node on FABRIC\n",
    "target_host=f'{ollama_node.get_username()}@{ollama_node.get_management_ip()}'\n",
    "\n",
    "print(\"Use `cd` to navigate into the `fabric_ssh_tunnel_tools` folder.\")\n",
    "print(\"In your terminal, run the SSH tunnel command\")\n",
    "print()\n",
    "print(f'ssh  -L {local_host}:{local_port}:127.0.0.1:{target_port} -i {os.path.basename(fablib.get_default_slice_public_key_file())[:-4]} -F ssh_config {target_host}')\n",
    "print()\n",
    "print(\"After running the SSH command, open Open WebUI at http://localhost:8080. If prompted, create an account and start asking questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the Slice\n",
    "\n",
    "Please delete your slice when you are done with your experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ollama_node = fablib.get_slice(ollama_slice_name)\n",
    "#ollama_node.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
